{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DehazeNet model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from detection_utils import load_yolo_model, draw_bounding_boxes\n",
    "\n",
    "# Check if GPU is available and use it, otherwise fallback to CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the DehazeNet model architecture\n",
    "class DehazeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DehazeNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.conv2 = torch.nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "# Load the DehazeNet model\n",
    "model_path = \"D:\\\\Mini project Git\\\\Dehazing-and-Human-Detection-Project\\\\Dehazing_model\\\\dehaze_model.pth\"\n",
    "model = DehazeNet().to(device)\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(\"DehazeNet model loaded successfully.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading the DehazeNet model: {e}\")\n",
    "model.eval()\n",
    "\n",
    "# Define the image transformation (same as used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Reverse the normalization after prediction\n",
    "def reverse_normalization(tensor):\n",
    "    return tensor * 0.5 + 0.5\n",
    "\n",
    "# Function to process frames through DehazeNet\n",
    "def process_frame(frame):\n",
    "    input_frame = transform(frame).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_frame).cpu()\n",
    "    output_frame = output.squeeze(0).permute(1, 2, 0).numpy()\n",
    "    output_frame = reverse_normalization(output_frame)\n",
    "    output_frame = np.clip(output_frame * 255, 0, 255).astype(np.uint8)\n",
    "    return output_frame\n",
    "\n",
    "# Load YOLO model\n",
    "config_path = \"D:\\\\Mini project Git\\\\Dehazing-and-Human-Detection-Project\\\\Human_Detection\\\\yolov3.cfg\"\n",
    "weights_path = \"D:\\\\Mini project Git\\\\Dehazing-and-Human-Detection-Project\\\\Human_Detection\\\\yolov3.weights\"\n",
    "labels = [\"person\", \"cat\", \"dog\"]\n",
    "net, output_layers = load_yolo_model(config_path, weights_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert BGR to RGB for processing\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Dehaze the frame\n",
    "    dehazed_frame = process_frame(frame_rgb)\n",
    "\n",
    "    # YOLO object detection\n",
    "    blob = cv2.dnn.blobFromImage(dehazed_frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    boxes, confidences, class_ids = [], [], []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id < len(labels) and labels[class_id] in [\"person\", \"cat\", \"dog\"]:\n",
    "                center_x, center_y = int(detection[0] * frame.shape[1]), int(detection[1] * frame.shape[0])\n",
    "                w, h = int(detection[2] * frame.shape[1]), int(detection[3] * frame.shape[0])\n",
    "                x, y = int(center_x - w / 2), int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply Non-Maximum Suppression\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    if len(indices) > 0:\n",
    "        indices = indices.flatten()\n",
    "        boxes = [boxes[i] for i in indices]\n",
    "        confidences = [confidences[i] for i in indices]\n",
    "        class_ids = [class_ids[i] for i in indices]\n",
    "\n",
    "    # Count humans and animals\n",
    "    human_count = sum(1 for id in class_ids if labels[id] == \"person\")\n",
    "    animal_count = sum(1 for id in class_ids if labels[id] in [\"cat\", \"dog\"])\n",
    "\n",
    "    # Convert dehazed frame back to BGR for OpenCV\n",
    "    dehazed_frame_bgr = cv2.cvtColor(dehazed_frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    annotated_frame = draw_bounding_boxes(dehazed_frame_bgr, boxes, confidences, class_ids, labels)\n",
    "\n",
    "    # Display counts\n",
    "    cv2.putText(annotated_frame, f\"Humans: {human_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(annotated_frame, f\"Animals: {animal_count}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Show the annotated frame\n",
    "    cv2.imshow(\"Dehazed Frame with Detection\", annotated_frame)\n",
    "\n",
    "    # Exit on 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
